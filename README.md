# Classifying-fashion-images
Unleashing the Power of Deep Learning: Fashion Image Classification with Feed-Forward Neural Networks

**Introduction-**
In today's era of rapid technological advancement, artificial intelligence (AI) and machine learning (ML) continue to revolutionize various industries, from healthcare to finance and beyond. One area where these technologies have made significant strides is in computer vision, particularly in image classification tasks.
In this blog post, we embark on a journey into the realm of deep learning, focusing on one of its fundamental architectures: the feed-forward neural network. Specifically, we explore how feed-forward neural networks can be leveraged to tackle the fascinating challenge of classifying fashion images.
Fashion image classification, a subset of computer vision tasks, involves categorizing images of clothing items into predefined classes or categories. Whether it's identifying a dress, or a pair of shoes, the ability to automatically classify fashion images has profound implications for e-commerce, retail, and fashion industry applications.
At the heart of this endeavor lies the feed-forward neural network, a foundational architecture in the field of deep learning. The feed-forward neural network operates in a straightforward manner, making it an ideal starting point for understanding neural networks. In these types of neural networks, the information flows from one layer to another, in one direction, from the input layer to one or more hidden layers and then the output layer. 
Throughout this blog post, we'll unravel the inner workings of feed-forward neural networks, from pre-processing fashion image data to training and evaluating a model capable of achieving a minimum accuracy of 88% in classifying clothing items. Along the way, we'll delve into essential concepts such as feature extraction, model architecture design, activation functions, and optimization algorithms.
So, join us as we delve into the fascinating world of deep learning and witness first-hand the transformative power of feed-forward neural networks in fashion image classification. Whether you're a novice enthusiast eager to learn or a seasoned practitioner seeking insights, this blog post promises to enlighten and inspire your journey in the captivating realm of artificial intelligence and computer vision.

**Reading the data –**
For this project I am using  the fashion MNIST dataset. The Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. The classes are 0: T-shirt/top,  1: Trouser and so on until 9. I imported the Fashion MNIST dataset using tensorflow.keras. 
 
**Feature scaling/standardization –** 
Since we have  8-bit grayscale images, the pixel intensities go from 0 to 255, where 0 is black and 255 is white. In order for us to fit the features into the Neural Network, improve the training process and the performance of the model, we need to scale these features in a way that all the values are in the range of [0,1]. Scaling the pixel values to a similar range [0, 1] helps ensure numerical stability during the training process and prevents issues such as vanishing or exploding gradients. In this case, I have divided the values of both X_train and X_test by 255, in order to scale the features. 
 

I could have also used the MinMaxScaler() transformation to standardize the pixels of the images, which also helps us scale the data in the range of (0,1).

**Creating features  from the pixels–** 
To optimize image data for neural networks, reshaping the input is crucial. In the case of images presented in a 28x28 format, reshaping transforms each pixel into a single feature. This restructuring effectively converts the 2D image grid into a 1D feature vector, with dimensions determined by the image's width and height (e.g., 28x28 images yield 784-dimensional vectors). This process ensures that each pixel contributes as a distinct input feature to the neural network, enhancing its ability to understand and classify images.
  
**Fitting a feed-forward architecture NN–** 
In this paragraph, we'll delve into the significance of each component of the neural network architecture, including the choice of cost function, activation functions, number of layers, and neurons in each layer. This exploration aims to provide insights into the thought process behind designing a network tailored for achieving optimal performance in image classification tasks.
In the journey towards achieving a minimum accuracy of 88% on the test dataset, the next critical step involves fitting a feed-forward neural network to the scaled data. 
I have fit a neural network with an input layer consisting of 784 neurons, as explained above. After that I have added 2 hidden layers I have also added the activation: Rectified Linear Unit (ReLU) on both the hidden layers. After each hidden layer, I have added a dropout layer of 10%. Finally, I added the output layer with 10 neurons (10 output classes) , with an activation of “Softmax”. Let's dissect the architecture of the network and elucidate the rationale behind each decision:
•	Activations – 
o	ReLU (Rectified Linear Unit): ReLU is chosen as the activation function for the hidden layers . ReLU has been widely adopted in deep learning due to its simplicity and effectiveness in combating the vanishing gradient problem. ReLU is particularly efficient in sparse representations and accelerates the convergence of gradient-based optimization algorithms. It also allows for very fast computations.
o	Softmax: Softmax activation is used in the output layer. Softmax transforms the raw output scores into class probabilities, ensuring that the sum of the probabilities across all classes equals one. This activation function is suitable for multi-class classification tasks (like this one) as it provides a probability distribution over the classes, facilitating interpretation and decision-making.

•	Dropout layers - Dropout layers with a dropout rate of 0.1(10%) are inserted after each dense layer. Dropout is a regularization technique that randomly sets a fraction of input units to zero during training, effectively "dropping out" some neurons. This helps prevent overfitting by reducing the interdependence of neurons and forcing the network to learn more robust features. By introducing noise and redundancy, dropout encourages the network to learn a diverse set of representations, improving its generalization performance on unseen data.

•	Number of Layers and Neurons -  The network comprises two hidden layers with 250 and 100 neurons, respectively. The choice of two hidden layers allows the network to capture hierarchical representations of the input data, gradually learning intricate features from low-level edges to high-level concepts. Increasing the depth of the network enables it to learn more complex patterns and improve its capacity to represent the underlying data distribution.

In order to configure(compile) the Neural Network for training I have used:
•	Optimizer: Adam
o	Adam is a popular optimization algorithm widely used in training neural networks. Adam maintains adaptive learning rates for each parameter, adjusting them dynamically based on the gradients' past momentum and the second moments (moving averages) of the gradients. This adaptiveness often leads to faster convergence and improved generalization performance.

•	Loss Function:   Sparse Categorical Cross entropy
o	The choice of loss function depends on the nature of the task and the output representation. In this case, since the problem is a multi-class classification task where the labels are integers (0, 1, 2, ..., 9) and not One-hot encoded, "sparse_categorical_crossentropy" is an appropriate choice. This loss function calculates the cross-entropy between the true labels and the predicted probabilities, considering the target labels as integers rather than one-hot encoded vectors. It penalizes misclassifications and guides the model towards learning accurate class probabilities.

•	Metrics: Accuracy
o	The accuracy metric measures the proportion of correctly classified samples over the total number of samples in the dataset. It provides an intuitive measure of the model's performance in terms of correct classifications. By monitoring accuracy during training, we can track the model's progress and assess its ability to classify images correctly.
Results:
After fitting the feed-forward neural network to the scaled data and training the model, we achieved an impressive accuracy of 0.90 (90%) on the test dataset. This outcome surpasses our initial goal of obtaining at least an 88% accuracy, underscoring the effectiveness of the chosen architecture and training approach.

**Conclusion:**
In this exploration of fashion image classification using feed-forward neural networks, we embarked on a journey into the captivating realm of deep learning. From pre-processing the Fashion MNIST dataset to designing and training a neural network model, we unravelled the inner workings of neural networks and their application in computer vision tasks. Through meticulous architectural choices, including the use of ReLU activation functions, Dropout regularization, and the Adam optimizer, we crafted a robust model capable of accurately classifying clothing items with remarkable precision. The journey illuminated essential concepts such as feature engineering, model architecture design, and optimization algorithms, providing valuable insights for practitioners seeking to leverage deep learning in image classification tasks.

**Suggestions for Practitioners:**
As practitioners continue their foray into deep learning and image classification, several avenues for exploration and refinement emerge. Firstly, experimenting with different network architectures, such as convolutional neural networks (CNNs), may yield further performance improvements, particularly for tasks involving image data. Additionally, fine-tuning hyperparameters, such as learning rate and dropout rates, could enhance the model's generalization ability and convergence speed. Furthermore, incorporating data augmentation techniques, such as rotation, scaling, and flipping, may augment the training dataset and improve the model's robustness to variations in input data. Lastly, staying abreast of advancements in deep learning research and exploring emerging techniques, can further enrich the practitioner's toolkit and propel the field of computer vision forward. 
In summary, the journey into fashion image classification with feed-forward neural networks has illuminated the transformative power of deep learning in solving real-world challenges.
